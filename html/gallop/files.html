<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>gallop.files API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gallop.files</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pymatgen as pmg
from pymatgen.symmetry import groups
from pymatgen.io import cif
from pymatgen.core.operations import SymmOp
from pymatgen.io.cif import CifBlock, CifFile
from collections import OrderedDict
from monty.io import zopen
import time
import os
import pickle

import gallop.zm_to_cart as zm_to_cart

def get_data_from_DASH_sdi(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH .sdi file to obtain numpy arrays of:
        - hkls
        - twotheta for hkls
        - peak intensities
        - inverse covariance matrix
    As well as:
        - pymatgen lattice
        - pymatgen Space group

    Args:
        filename (str): The filename of the .sdi to read in. The associated
            hcv, dsl and tic files should also be in the same directory and
            accessible.
        percentage_cutoff (int, optional):  the minimum percentage correlation
                        to be included in the inverse covariance matrix.
                        Defaults to 20 to be comparable with DASH, however, this
                        doesn&#39;t affect the speed of GALLOP on GPU so can be
                        freely set without impacting performance.
    Returns:
        dict: A dictionary with the required information
    &#34;&#34;&#34;
    data = {}
    data[&#34;hcv&#34;], data[&#34;tic&#34;], data[&#34;dsl&#34;], data[&#34;unit_cell&#34;], \
                data[&#34;sg_number&#34;], data[&#34;sg_setting&#34;] = read_DASH_sdi(filename)
    data[&#34;wavelength&#34;] = read_DASH_dsl(data[&#34;dsl&#34;])
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    data[&#34;hkl&#34;], data[&#34;intensities&#34;], data[&#34;sigma&#34;],\
        data[&#34;inverse_covariance_matrix&#34;] = read_DASH_hcv(
            data[&#34;hcv&#34;], percentage_cutoff_inv_cov=percentage_cutoff_inv_cov)
    data[&#34;twotheta&#34;] = read_DASH_tic(data[&#34;tic&#34;])
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(data[&#34;unit_cell&#34;][0],
                                                    data[&#34;unit_cell&#34;][1],
                                                    data[&#34;unit_cell&#34;][2],
                                                    data[&#34;unit_cell&#34;][3],
                                                    data[&#34;unit_cell&#34;][4],
                                                    data[&#34;unit_cell&#34;][5])
    data[&#34;space_group&#34;] = groups.SpaceGroup.from_int_number(data[&#34;sg_number&#34;])
    return data


def get_DASH_inverse_covariance_matrix(
                    off_diag_elements, sigma, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH-produced list of correlated peaks and produce
    a 2D numpy inverse covariance matrix.
        -   Populate the diagonals with sigma^2
        -   Populate the off-diagonals as raw decimals rather than
            percentages to save having to do it later.

    Args:
        off_diag_elements (numpy array): Non-zero off-diagonal elements
            of the inverse correlation matrix stored in the hcv, which
            will be converted into the covariance matrix
        sigma (numpy array): The square-root of the diagonal elements of
            the inverse covariance matrix
        percentage_cutoff_inv_cov (int, optional): the minimum percentage
            correlation to be included in the inverse covariance matrix.
            Defaults to 20 to be comparable with DASH, however, this doesn&#39;t
            affect the speed of GALLOP so can be freely set without impacting
            performance.

    Returns:
        numpy array: The inverse covariance matrix
    &#34;&#34;&#34;
    matrix = np.zeros((len(off_diag_elements), len(off_diag_elements)))
    for i in range(0,len(off_diag_elements)):
        if i != len(off_diag_elements)-1:
            j = i+1
            # Populate diagonals
            matrix[i][i] += sigma[i]**2
            # Populate off diagonal elements
            for corr in off_diag_elements[i]:
                if float(corr) &gt;= percentage_cutoff_inv_cov:
                    matrix[i][j] = 0.01*sigma[i]*sigma[j]*float(corr)
                    matrix[j][i] += matrix[i][j]
                j+=1
        else:
            matrix[i][i] += sigma[i]**2
    return matrix


def read_DASH_hcv(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH-produced hcv, and return numpy arrays of hkl,
    intensity, sigma values together with a 2D numpy inverse_covariance
    matrix.
    Format of DASH hcv files:
    h, k, l, I, sigma, count, off-diag inv. of correlation matrix
    See function get_DASH_inverse_covariance_matrix for how the
    inverse_covariance_matrix is populated using the sigma and
    inverse correlation matrix values.

    Args:
        filename (str): Name for the DASH .sdi file. File is assumed
            to be in the working directory
        percentage_cutoff_inv_cov (int, optional): the minimum percentage
            correlation to be included in the inverse covariance
            matrix. Defaults to 20 to be comparable with DASH,
            however, this doesn&#39;t affect the speed of GALLOP so can
            be set as desired without impacting performance.

    Returns:
        tuple:  Numpy arrays of hkl, intensity, sigma and the inverse
                covariance matrix
    &#34;&#34;&#34;

    hkl = []
    I = []
    sigma = []
    inverse_covariance_off_diag = []
    peaknumbers = []
    with open(filename) as in_hcv:
        for line in in_hcv:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            hkl.append(line[0:3])
            I.append(line[3])
            sigma.append(line[4])
            peaknumbers.append(int(line[5]))
            inverse_covariance_off_diag.append(line[6:])
    in_hcv.close()
    hkl = np.array(hkl).astype(int)
    I = np.array(I).astype(float)
    sigma = np.array(sigma).astype(float)
    peaknumbers = np.array(peaknumbers)
    inverse_covariance = get_DASH_inverse_covariance_matrix(
                                inverse_covariance_off_diag,
                                sigma,
                                percentage_cutoff_inv_cov=percentage_cutoff_inv_cov
                                )
    return hkl, I, sigma, inverse_covariance

def read_DASH_dsl(filename):
    &#34;&#34;&#34;
    Read a DASH dsl file to obtain the wavelength used

    Args:
        filename (str): Filename of the dsl file

    Returns:
        float: the wavelength used
    &#34;&#34;&#34;
    with open(filename) as in_dsl:
        for line in in_dsl:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if &#34;rad&#34; in line:
                return float(line[1])

def read_DASH_tic(filename):
    &#34;&#34;&#34;
    Read in a DASH-produced tic file, and return numpy array of
    twotheta values

    Args:
        filename (str): Filename of the .tic file

    Returns:
        Numpy array: the two-theta values for each hkl
    &#34;&#34;&#34;
    twotheta = []
    with open(filename) as in_tic:
        for line in in_tic:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            twotheta.append(line[3])
    in_tic.close()
    twotheta = np.array(twotheta).astype(float)
    return twotheta

def read_DASH_sdi(filename):
    &#34;&#34;&#34;
    Read a DASH .sdi file, and obtain the filenames for the the .hcv
    and .tic, as well as the unit cell lattice parameters and the
    space group.

    Args:
        filename (str): Filename of the DASH .sdi file

    Returns:
        tuple: contains the filenames of hcv, tic files as well as
            the unit cell, spacegroup and its setting.
    &#34;&#34;&#34;
    directory = os.path.split(filename)[0]
    with open(filename) as in_sdi:
        for line in in_sdi:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if line[0] == &#34;HCV&#34;:
                hcv = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;TIC&#34;:
                tic = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;DSL&#34;:
                dsl = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;Cell&#34;:
                unit_cell = np.array(line[1:]).astype(float)
            if line[0] == &#34;SpaceGroup&#34;:
                sg = int(line[2].split(&#34;:&#34;)[0])
                sg_setting = line[2].split(&#34;:&#34;)[-1]
    in_sdi.close()
    return hcv, tic, dsl, unit_cell, sg, sg_setting


def get_data_from_GSAS_gpx(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a GSAS project file (.gpx) to obtain numpy arrays of:
        - hkls
        - peak intensities
        - inverse covariance matrix
    As well as:
        - pymatgen lattice
        - pymatgen Space group

    Args:
        filename (str): [description]
        percentage_cutoff_inv_cov (int, optional):  the minimum percentage
                        correlation to be included in the inverse
                        covariance matrix. Defaults to 20 to be
                        comparable with DASH, however, this doesn&#39;t
                        affect the speed of GALLOP so can be freely
                        set without impacting performance.
    Returns:
        dict: A dictionary with the required information
    &#34;&#34;&#34;
    data, names = IndexGPX(filename)

    instrument_params = data[names[-2][0]][names[-2][4]][0]
    if &#34;Lam&#34; in instrument_params.keys():
        wavelength = instrument_params[&#34;Lam&#34;][1]
    elif &#34;Lam1&#34; in instrument_params.keys():
        wavelength = instrument_params[&#34;Lam1&#34;][1]

    space_group = data[&#34;Phases&#34;][names[-1][-1]][&#34;General&#34;][&#34;SGData&#34;][&#34;SpGrp&#34;]
    cell = data[&#34;Phases&#34;][names[-1][-1]][&#34;General&#34;][&#34;Cell&#34;][1:-1]

    hkl, dspacing, I, sigma = [], [], [], []
    for d in data[&#34;Phases&#34;][names[-1][-1]][&#34;Pawley ref&#34;]:
        hkl.append(d[:3])
        dspacing.append(d[4])
        I.append(d[-2])
        sigma.append(d[-1])

    hkl = np.array(hkl).astype(int)
    dspacing = np.array(dspacing)
    I = np.array(I)
    sigma = np.array(sigma)

    names_subset = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;varyList&#34;]
    names_all = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;varyListStart&#34;]
    cov_subset = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;covMatrix&#34;]

    intensities, inverse_covariance_matrix = get_GSAS_inv_cov_matrix(cov_subset,
                            names_all, names_subset, I,
                            percentage_cutoff_inv_cov=percentage_cutoff_inv_cov)
    data = {}
    data[&#34;wavelength&#34;] = wavelength
    data[&#34;hkl&#34;] = hkl
    data[&#34;intensities&#34;] = intensities
    data[&#34;dspacing&#34;] = dspacing
    data[&#34;inverse_covariance_matrix&#34;] = inverse_covariance_matrix
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(cell[0], cell[1], cell[2],
                                                    cell[3], cell[4], cell[5])
    data[&#34;space_group&#34;] = pmg.symmetry.groups.SpaceGroup(space_group)
    data[&#34;sg_number&#34;] = int(data[&#34;space_group&#34;].int_number)
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    return data

def IndexGPX(GPXfile):
    &#39;&#39;&#39;
    Pinched from the GSAS-II source code, and modified to allow it to work
    without needing extra GSAS related imports. See original source here:
    https://gsas-ii.readthedocs.io/en/latest/_modules/GSASIIstrIO.html#IndexGPX

    Original docstring below:
    &gt;    Create an index to a GPX file, optionally the file into memory.
    &gt;    The byte size of the GPX file is saved. If this routine is called
    &gt;    again, and if this size does not change, indexing is not repeated
    &gt;    since it is assumed the file has not changed (this can be overriden
    &gt;    by setting read=True).
    &gt;
    &gt;    :param str GPXfile: full .gpx file name
    &gt;    :returns: Project,nameList if read=, where
    &gt;
    &gt;     * Project (dict) is a representation of gpx file following the GSAS-II
    &gt;       tree structure for each item: key = tree name (e.g. &#39;Controls&#39;,
    &gt;       &#39;Restraints&#39;, etc.), data is dict
    &gt;     * nameList (list) has names of main tree entries &amp; subentries used to
    &gt;       reconstruct project file
    &#39;&#39;&#39;
    gpxIndex = {}
    gpxNamelist = []
    fp = open(GPXfile,&#39;rb&#39;)
    Project = {}
    try:
        while True:
            pos = fp.tell()
            data = pickle.load(fp)
            datum = data[0]
            gpxIndex[datum[0]] = pos
            Project[datum[0]] = {&#39;data&#39;:datum[1]}
            gpxNamelist.append([datum[0],])
            for datus in data[1:]:
                Project[datum[0]][datus[0]] = datus[1]
                gpxNamelist[-1].append(datus[0])
    except EOFError:
        pass
    fp.close()
    return Project, gpxNamelist

def get_GSAS_inv_cov_matrix(cov_subset, names_all, names_subset, I,
                                                percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Inverts the covariance matrix from GSAS, and use this to rebuild a full
    inverse covariance matrix that includes the hkls that were excluded in
    the GSAS-II Pawley fitting procedure (due to overlap/equivalence etc)

    Args:
        cov_subset (np.array): The covariance matrix from GSAS. This will be of
                        shape names_subset x names_subset
        names_all (list): A list of all of the peaks within the data range used
                        for the Pawley fit in GSAS
        names_subset (list): The non-equivalent peaks used by GSAS to perform
                        the Pawley refinement
        percentage_cutoff_inv_cov (int, optional):  the minimum percentage
                        correlation to be included in the inverse
                        covariance matrix. Defaults to 20 to be comparable to
                        DASH. There is no difference in speed between 0 and 20
                        (or any other number from 0-100) so can be tested freely
                        without impacting run times.

    Returns:
        np.array: The modified intensity for all hkls
        np.array: The full inverse covariance matrix for all hkls
    &#34;&#34;&#34;

    # Find the peaks not in the Pawley refinement and account for any other
    # refined variables, as we only want the peak-to-peak covariance
    peak_indices = [i for i, x in enumerate(names_subset) if &#34;PWLref&#34; in x]

    names_all = [x for x in names_all if &#34;PWL&#34; in x]
    names_subset = [x for x in names_subset if &#34;PWL&#34; in x]
    missing_peaks = [x for x in names_all if x not in names_subset]

    # Invert the covariance matrix and convert to the inverse correlation matrix
    inv_cov_subset = np.linalg.inv(cov_subset)
    # Select only the peak-to-peak inverse covariances
    inv_cov_subset = inv_cov_subset[peak_indices][:,peak_indices]
    inv_sigma_subset = np.sqrt(np.diag(inv_cov_subset))
    inv_cor_subset = (np.diag(1/inv_sigma_subset)
                        @ inv_cov_subset
                        @ np.diag(1/inv_sigma_subset))

    # Now we have the inverse correlation rather than covariance matrix,
    # we can build it up to include all of the peaks, then rebuild the inverse
    # covariance matrix from that. We also need to modify the intensities and
    # 1/sigmas of the peaks that were excluded from the Pawley fit.
    # Effectively, this is converting the output to look like a DASH hcv, from
    # which the same logic is then used to rebuild the inverse covariance matrix
    inv_cor_full = []
    inv_sigma_full = []
    I_mod = np.copy(I)
    j=0
    for i, x in enumerate(names_all):
        if x not in missing_peaks:
            correlations = inv_cor_subset[i-j][i-j+1:]
            inv_cor_full.append(np.around(correlations, 2).tolist())
            inv_sigma_full.append(inv_sigma_subset[i-j])
        else:
            inv_sigma_full.append(None)
            inv_cor_full.append(None)
            j+=1

    for i, s in enumerate(inv_sigma_full):
        if s is None:
            inv_sigma_full[i] = inv_sigma_full[i-1]

    inv_sigma_mod = np.copy(inv_sigma_full)
    for i, c in enumerate(inv_cor_full):
        if c is not None:
            k = 0
            if i &lt; (len(inv_cor_full)-1):
                while inv_cor_full[i+k+1] is None:
                    if i + k + 2 == len(inv_cor_full):
                        break
                    k+=1
                if k &gt; 0:
                    base_correlations = inv_cor_full[i]
                    for j in range(k+1):
                        inv_cor_full[i+j] = (k-j)*[1.] + base_correlations
                        # Divide the intensity of the overlapped peaks by the
                        # number of peaks that were overlapped
                        I_mod[i+j] = I_mod[i+j] / (k+1.)
                        # Multiply the inverse sigma of the overlapped peaks by
                        # the number of peaks that were overlapped
                        inv_sigma_mod[i+j] = inv_sigma_mod[i+j] * (k+1.)

    # Now reconstruct the full inverse correlation matrix
    inv_cor_matrix = np.zeros((len(inv_cor_full), len(inv_cor_full)))
    for i in range(0,len(inv_cor_full)):
        if i != len(inv_cor_full)-1:
            j = i+1
            # Populate diagonals
            inv_cor_matrix[i][i] = 1.
            # Populate off diagonal elements
            for corr in inv_cor_full[i]:
                if np.abs(corr) &gt;= (percentage_cutoff_inv_cov/100.):
                    inv_cor_matrix[i][j] = float(corr)
                    inv_cor_matrix[j][i] = inv_cor_matrix[i][j]
                j+=1
        else:
            inv_cor_matrix[i][i] = 1.

    # Finally, get the inverse covariance matrix needed for chi2 calcs.
    inverse_covariance_matrix = (np.diag(inv_sigma_mod)
                                @ inv_cor_matrix
                                @ np.diag(inv_sigma_mod))
    return I_mod, inverse_covariance_matrix

def get_TOPAS_matrix(lines):
    &#34;&#34;&#34;
    Read some lines from a TOPAS output file that correspond to
    a Covariance matrix and return a numpy array of the matrix

    Args:
        lines (List): List of the lines in a TOPAS output file

    Returns:
        numpy array: The topas covariance matrix
    &#34;&#34;&#34;
    matrix = []
    for line in lines:
        temp = []
        for item in line[2:]:
            if &#34;iprm&#34; in line[0]:
                try:
                    temp.append(float(item))
                except ValueError:
                    minus_idx = [i for i, ltr in enumerate(item) if ltr == &#34;-&#34;]
                    split = item.split(&#34;-&#34;)
                    if len(minus_idx) &gt; 1:
                        temp.append(-1*float(split[1]))
                        temp.append(-1*float(split[2]))
                    else:
                        temp.append(float(split[0]))
                        temp.append(-1*float(split[1]))
        matrix.append(temp)
    return np.array(matrix)

def get_data_from_TOPAS_output(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Note: This assumes that the only refined parameters in the Pawley refinement
    are the peak intensities.
    Args:
        filename ([type]): [description]
        percentage_cutoff_inv_cov (int, optional): [description]. Defaults to 20

    Returns:
        dict: A dictionary of the required information
    &#34;&#34;&#34;

    lines = []
    with open(filename, &#34;r&#34;) as in_file:
        for line in in_file:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if len(line) &gt; 0:
                lines.append(line)
    in_file.close()

    cell = {}
    hkl = []
    m = []
    dspacing = []
    twotheta = []
    I = []
    #sigma = []
    C_matrix = []
    gof = 1
    for i, line in enumerate(lines):
        if &#34;la&#34; in line and &#34;1&#34; in line and &#34;lo&#34; in line:
            wavelength = line[-1]
        if len(line) == 2:
            if &#34;a&#34; in line:
                cell[&#34;a&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;b&#34; in line:
                cell[&#34;b&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;c&#34; in line:
                cell[&#34;c&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;al&#34; in line:
                cell[&#34;al&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;be&#34; in line:
                cell[&#34;be&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;ga&#34; in line:
                cell[&#34;ga&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
        if &#34;space_group&#34; in line:
            cell[&#34;space_group&#34;] = &#34; &#34;.join(line[1:]).strip(&#34;\&#34;&#34;)
        if &#34;gof&#34; in line:
            gof *= float(line[1])
        if len(line) == 8 and &#34;@&#34; in line:
            hkl.append([int(x) for x in line[0:3]])
            m.append(int(line[3]))
            dspacing.append(line[4])
            twotheta.append(line[5])
            I_sig = line[-1].split(&#34;`_&#34;)
            I.append(float(I_sig[0]))
            #sigma.append(float(I_sig[1]))
        else:
            if &#34;C_matrix&#34; in line:
                n_peaks = int(lines[i+2][-1])
                C_matrix = get_TOPAS_matrix(lines[i+3:i+2+n_peaks+1])

    hkl = np.array(hkl)
    m = np.array(m)
    dspacing = np.array(dspacing)
    twotheta = np.array(twotheta)
    I = np.array(I)
    I_mult_corrected = I/m
    C_matrix_mult_corrected = np.diag(1/m) @ C_matrix @ np.diag(1/m)
    #npeaks = len(hkl)
    #while not np.all(
    #    np.linalg.eigvals(
    #    np.linalg.inv(C_matrix_mult_corrected[:npeaks][:,:npeaks])) &gt; 0):
    #    npeaks -= 1
    #if npeaks &lt; len(hkl):
    #    print(&#34;WARNING - inverse covariance matrix is not positive definite.&#34;)
    #    print(&#34;Recommend reducing peak count to&#34;,npeaks)
    #    print(&#34;This is&#34;,round(100*float(npeaks)/len(hkl),2),&#34;% of the peaks &#34;
    #            &#34;in the Pawley file&#34;)

    ############################################################################
    # Still need to work out how to implement the % correlation cutoff
    # It&#39;s proving difficult because the matrices from TOPAS I&#39;ve got give
    # negative values on the inverse matrix diagonal which prevents me from
    # converting to the inverse correlation matrix.
    ############################################################################
    data = {}
    data[&#34;percentage_cutoff_inv_cov&#34;] = percentage_cutoff_inv_cov
    data[&#34;hkl&#34;] = hkl.astype(int)
    data[&#34;intensities&#34;] = I_mult_corrected
    data[&#34;inverse_covariance_matrix&#34;] = np.linalg.inv(C_matrix_mult_corrected)
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(cell[&#34;a&#34;],
                                                    cell[&#34;b&#34;],
                                                    cell[&#34;c&#34;],
                                                    cell[&#34;al&#34;],
                                                    cell[&#34;be&#34;],
                                                    cell[&#34;ga&#34;])
    space_group = pmg.symmetry.groups.SpaceGroup(cell[&#34;space_group&#34;])

    data[&#34;sg_number&#34;] = int(space_group.int_number)
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    data[&#34;space_group&#34;] = space_group
    data[&#34;dspacing&#34;] = dspacing
    data[&#34;twotheta&#34;] = twotheta
    data[&#34;wavelength&#34;] = wavelength
    return data


def get_CIF_atomic_coords_and_species(Structure, filename,
                                        add_CIF_dw_factors=True):
    &#34;&#34;&#34;
    Read a CIF and extract the fractional atomic-coordinates.
    It is assumed that a GALLOP Structure object has been created
    that already contains the correct unit cell and PXRD Pawley data.

    These coordinates are added to the Structure object.
    Args:
        Structure (class): GALLOP structure object
        filename (str): Filename of the CIF. Assumed to be in the working
            directory.
        add_CIF_dw_factors (bool, optional): Include Debye-Waller factors
            for the atoms in the CIF. By default, non-H = 3, H = 6.
            Defaults to True.
    &#34;&#34;&#34;

    print(&#34;Ensure that CIF and structure fit files are in the same setting&#34;)
    cif_file_structure = cif.CifParser(filename).as_dict()
    cif_file_structure = cif_file_structure[list(cif_file_structure.keys())[0]]
    Structure.cif_species = cif_file_structure[&#34;_atom_site_type_symbol&#34;]
    x = cif_file_structure[&#34;_atom_site_fract_x&#34;]
    y = cif_file_structure[&#34;_atom_site_fract_y&#34;]
    z = cif_file_structure[&#34;_atom_site_fract_z&#34;]

    x = np.array([v.split(&#34;(&#34;)[0] for v in x]).astype(float)
    y = np.array([v.split(&#34;(&#34;)[0] for v in y]).astype(float)
    z = np.array([v.split(&#34;(&#34;)[0] for v in z]).astype(float)
    Structure.cif_frac_coords = np.vstack([x,y,z]).T
    if add_CIF_dw_factors:
        elements = list(set(Structure.cif_species))
        Structure.CIF_dw_factors = {}
        for e in elements:
            if e != &#34;H&#34;:
                Structure.CIF_dw_factors[e] = 3.
            else:
                Structure.CIF_dw_factors[e] = 6.
    no_H_coords = []
    no_H_species = []
    if Structure.ignore_H_atoms:
        for i, e in enumerate(cif_file_structure[&#34;_atom_site_type_symbol&#34;]):
            if e != &#34;H&#34;:
                no_H_species.append(e)
                no_H_coords.append(Structure.cif_frac_coords[i])
        Structure.cif_species_no_H = no_H_species
        Structure.cif_frac_coords_no_H = np.vstack(no_H_coords)

class DASHCifWriter:
    &#34;&#34;&#34;
    A modified wrapper around the pymatgen CifFile to write CIFs from pymatgen
    structures.

    See &#34;CifWriter&#34; on this page for the original code:
    https://pymatgen.org/pymatgen.io.cif.html

    Modification is to stop the unit cell (and hence coordinates) from being
    &#34;standardised&#34;. This means that the output CIF is in the same setting as
    the input data which allows for easier comparison etc.
    &#34;&#34;&#34;
    def __init__(self, struct, symprec=None, significant_figures=8, sg_number=1,
                comment=None):
        &#34;&#34;&#34;
        Args:
            struct (Structure): structure to write
            symprec (float): If not none, finds the symmetry of the structure
                and writes the cif with symmetry information. Passes symprec
                to the SpacegroupAnalyzer.
            significant_figures (int): Specifies precision for formatting of
                floats. Defaults to 8.
            angle_tolerance (float): Angle tolerance for symmetry finding.
                Passes angle_tolerance to the SpacegroupAnalyzer.
                Used only if symprec is not None.
        &#34;&#34;&#34;

        format_str = &#34;{:.%df}&#34; % significant_figures

        block = OrderedDict()
        loops = []

        spacegroup = (pmg.symmetry.groups.sg_symbol_from_int_number(sg_number),
                        str(sg_number))

        latt = struct.lattice
        comp = struct.composition
        no_oxi_comp = comp.element_composition
        block[&#34;_symmetry_space_group_name_H-M&#34;] = spacegroup[0]
        for cell_attr in [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]:
            block[&#34;_cell_length_&#34; + cell_attr] = format_str.format(
                getattr(latt, cell_attr))
        for cell_attr in [&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;]:
            block[&#34;_cell_angle_&#34; + cell_attr] = format_str.format(
                getattr(latt, cell_attr))
        block[&#34;_symmetry_Int_Tables_number&#34;] = spacegroup[1]
        block[&#34;_chemical_formula_structural&#34;] = no_oxi_comp.reduced_formula
        block[&#34;_chemical_formula_sum&#34;] = no_oxi_comp.formula
        block[&#34;_cell_volume&#34;] = format_str.format(latt.volume)

        _, fu = no_oxi_comp.get_reduced_composition_and_factor()
        block[&#34;_cell_formula_units_Z&#34;] = str(int(fu))

        if symprec is None:
            block[&#34;_symmetry_equiv_pos_site_id&#34;] = [&#34;1&#34;]
            block[&#34;_symmetry_equiv_pos_as_xyz&#34;] = [&#34;x, y, z&#34;]
        else:
            symmops = []
            sg_symbol = pmg.symmetry.groups.sg_symbol_from_int_number(sg_number)
            SG = pmg.symmetry.groups.SpaceGroup(sg_symbol)
            for op in SG.symmetry_ops:
                v = op.translation_vector
                symmops.append(SymmOp.from_rotation_and_translation(
                    op.rotation_matrix, v))

            ops = [op.as_xyz_string() for op in symmops]
            block[&#34;_symmetry_equiv_pos_site_id&#34;] = \
                [&#34;%d&#34; % i for i in range(1, len(ops) + 1)]
            block[&#34;_symmetry_equiv_pos_as_xyz&#34;] = ops

        loops.append([&#34;_symmetry_equiv_pos_site_id&#34;,
                    &#34;_symmetry_equiv_pos_as_xyz&#34;])

        try:
            symbol_to_oxinum = OrderedDict([
                (el.__str__(),
                float(el.oxi_state))
                for el in sorted(comp.elements)])
            block[&#34;_atom_type_symbol&#34;] = symbol_to_oxinum.keys()
            block[&#34;_atom_type_oxidation_number&#34;] = symbol_to_oxinum.values()
            loops.append([&#34;_atom_type_symbol&#34;,
                            &#34;_atom_type_oxidation_number&#34;])
        except (TypeError, AttributeError):
            symbol_to_oxinum = OrderedDict([(el.symbol, 0) for el in
                                            sorted(comp.elements)])

        atom_site_type_symbol = []
        atom_site_symmetry_multiplicity = []
        atom_site_fract_x = []
        atom_site_fract_y = []
        atom_site_fract_z = []
        atom_site_label = []
        atom_site_occupancy = []
        count = 0
        for site in struct:
            for sp, occu in sorted(site.species.items()):
                atom_site_type_symbol.append(sp.__str__())
                atom_site_symmetry_multiplicity.append(&#34;1&#34;)
                atom_site_fract_x.append(format_str.format(site.a))
                atom_site_fract_y.append(format_str.format(site.b))
                atom_site_fract_z.append(format_str.format(site.c))
                atom_site_label.append(&#34;{}{}&#34;.format(sp.symbol, count))
                atom_site_occupancy.append(occu.__str__())
                count += 1

        block[&#34;_atom_site_type_symbol&#34;] = atom_site_type_symbol
        block[&#34;_atom_site_label&#34;] = atom_site_label
        block[&#34;_atom_site_symmetry_multiplicity&#34;] = \
                                                atom_site_symmetry_multiplicity
        block[&#34;_atom_site_fract_x&#34;] = atom_site_fract_x
        block[&#34;_atom_site_fract_y&#34;] = atom_site_fract_y
        block[&#34;_atom_site_fract_z&#34;] = atom_site_fract_z
        block[&#34;_atom_site_occupancy&#34;] = atom_site_occupancy
        loops.append([&#34;_atom_site_type_symbol&#34;,
                    &#34;_atom_site_label&#34;,
                    &#34;_atom_site_symmetry_multiplicity&#34;,
                    &#34;_atom_site_fract_x&#34;,
                    &#34;_atom_site_fract_y&#34;,
                    &#34;_atom_site_fract_z&#34;,
                    &#34;_atom_site_occupancy&#34;])

        d = OrderedDict()
        d[comp.reduced_formula] = CifBlock(block, loops, comp.reduced_formula)
        comment_for_file = &#34;# Generated using pymatgen and GALLOP&#34;
        if comment is not None:
            comment_for_file += &#34;\n&#34; + comment
        self.cf = CifFile(d, comment=comment_for_file)

    @property
    def ciffile(self):
        &#34;&#34;&#34;
        Returns: CifFile associated with the CifWriter.
        &#34;&#34;&#34;
        return self.cf

    def __str__(self):
        &#34;&#34;&#34;
        Returns the cif as a string.
        &#34;&#34;&#34;
        return self.cf.__str__()

    def write_file(self, filename):
        &#34;&#34;&#34;
        Write the cif file.
        &#34;&#34;&#34;
        with zopen(filename, &#34;wt&#34;) as f:
            f.write(self.__str__())

def save_CIF_of_best_result(Structure, result, start_time=None,
    n_reflections=None, filename_root=None):
    &#34;&#34;&#34;
    Save a CIF of the best result obtained in a minimise run.
    Filename contains information about the run. For example, if the result
    is the first run, and the best chi_2 value is 50.1, 300 reflections were
    used in the chi_2 calculation and the time taken to obtain this solution was
    1.2 minutes, then the filename would be:

        filename_root_001_50.1_chisqd_300_refs_1.2_mins.cif

    Args:
        Structure (class): GALLOP structure object
        result (dict): Dictionary of results obtained by minimise
        start_time (time): time.time() when the runs began
        n_reflections (int, optional): the number of reflections used for chi_2
        calc. Defaults to None.
        filename_root (str, optional): If None, takes the name from the
            Structure. Root of the filename can be overwritten with this
            argument. Defaults to None.
    &#34;&#34;&#34;
    external = result[&#34;external&#34;]
    internal = result[&#34;internal&#34;]
    chi_2 = result[&#34;chi_2&#34;]
    run = result[&#34;GALLOP Iter&#34;] + 1
    if n_reflections is None:
        n_reflections = len(Structure.hkl)
    elif n_reflections &gt; len(Structure.hkl):
        n_reflections = len(Structure.hkl)
    # For the purpose of outputting a CIF, include the H-atoms back in. However,
    # the GALLOP runs may not be finished, so save whatever the parameter is set
    # to, then restore this setting once the CIF has been written.
    ignore_H_setting = Structure.ignore_H_atoms

    Structure.ignore_H_atoms = False

    # Save a CIF of the best particle found
    best_frac_coords = zm_to_cart.get_asymmetric_coords_from_numpy(Structure,
            external[chi_2 == chi_2.min()], internal[chi_2 == chi_2.min()],
            verbose=False)

    species = []
    for zm in Structure.zmatrices:
        species += zm.elements

    output_structure = pmg.Structure(lattice=Structure.lattice, species=species,
                                    coords=best_frac_coords[0][:len(species)])
    ext_comment = &#34;# GALLOP External Coords = &#34; + &#34;,&#34;.join(list(
                                external[chi_2 == chi_2.min()][0].astype(str)))
    int_comment = &#34;# GALLOP Internal Coords = &#34; + &#34;,&#34;.join(list(
                                internal[chi_2 == chi_2.min()][0].astype(str)))
    comment = ext_comment + &#34;\n&#34; + int_comment
    writer = DASHCifWriter(output_structure, symprec=1e-12,
                            sg_number=Structure.original_sg_number,
                            comment=comment)

    if filename_root is None:
        filename_root = Structure.name
    if start_time is not None:
        filename = (filename_root
                + &#34;_{:04d}_{:.3f}_chisqd_{}_refs_{:.1f}_mins.cif&#34;)
        filename = filename.format(run, chi_2.min(), n_reflections,
                            (time.time()-start_time)/60)
    else:
        filename = (filename_root
                + &#34;_{:04d}_{:.3f}_chisqd_{}_refs.cif&#34;)
        filename = filename.format(run, chi_2.min(), n_reflections)
    ciffile = writer.cf.data[list(writer.cf.data.keys())[0]]
    # Add the filename to the data_ string in the CIF to make it easier to
    # navigate multiple structures in Mercury
    ciffile.header = filename
    writer.cf.data = {filename : ciffile}
    writer.write_file(filename)
    # Restore the Structure ignore_H_atoms setting
    Structure.ignore_H_atoms = ignore_H_setting</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gallop.files.IndexGPX"><code class="name flex">
<span>def <span class="ident">IndexGPX</span></span>(<span>GPXfile)</span>
</code></dt>
<dd>
<div class="desc"><p>Pinched from the GSAS-II source code, and modified to allow it to work
without needing extra GSAS related imports. See original source here:
<a href="https://gsas-ii.readthedocs.io/en/latest/_modules/GSASIIstrIO.html#IndexGPX">https://gsas-ii.readthedocs.io/en/latest/_modules/GSASIIstrIO.html#IndexGPX</a></p>
<p>Original docstring below:</p>
<blockquote>
<p>Create an index to a GPX file, optionally the file into memory.
The byte size of the GPX file is saved. If this routine is called
again, and if this size does not change, indexing is not repeated
since it is assumed the file has not changed (this can be overriden
by setting read=True).</p>
<p>:param str GPXfile: full .gpx file name
:returns: Project,nameList if read=, where</p>
<pre><code>* Project (dict) is a representation of gpx file following the GSAS-II
  tree structure for each item: key = tree name (e.g. 'Controls',
  'Restraints', etc.), data is dict
* nameList (list) has names of main tree entries &amp; subentries used to
  reconstruct project file
</code></pre>
</blockquote></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def IndexGPX(GPXfile):
    &#39;&#39;&#39;
    Pinched from the GSAS-II source code, and modified to allow it to work
    without needing extra GSAS related imports. See original source here:
    https://gsas-ii.readthedocs.io/en/latest/_modules/GSASIIstrIO.html#IndexGPX

    Original docstring below:
    &gt;    Create an index to a GPX file, optionally the file into memory.
    &gt;    The byte size of the GPX file is saved. If this routine is called
    &gt;    again, and if this size does not change, indexing is not repeated
    &gt;    since it is assumed the file has not changed (this can be overriden
    &gt;    by setting read=True).
    &gt;
    &gt;    :param str GPXfile: full .gpx file name
    &gt;    :returns: Project,nameList if read=, where
    &gt;
    &gt;     * Project (dict) is a representation of gpx file following the GSAS-II
    &gt;       tree structure for each item: key = tree name (e.g. &#39;Controls&#39;,
    &gt;       &#39;Restraints&#39;, etc.), data is dict
    &gt;     * nameList (list) has names of main tree entries &amp; subentries used to
    &gt;       reconstruct project file
    &#39;&#39;&#39;
    gpxIndex = {}
    gpxNamelist = []
    fp = open(GPXfile,&#39;rb&#39;)
    Project = {}
    try:
        while True:
            pos = fp.tell()
            data = pickle.load(fp)
            datum = data[0]
            gpxIndex[datum[0]] = pos
            Project[datum[0]] = {&#39;data&#39;:datum[1]}
            gpxNamelist.append([datum[0],])
            for datus in data[1:]:
                Project[datum[0]][datus[0]] = datus[1]
                gpxNamelist[-1].append(datus[0])
    except EOFError:
        pass
    fp.close()
    return Project, gpxNamelist</code></pre>
</details>
</dd>
<dt id="gallop.files.get_CIF_atomic_coords_and_species"><code class="name flex">
<span>def <span class="ident">get_CIF_atomic_coords_and_species</span></span>(<span>Structure, filename, add_CIF_dw_factors=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Read a CIF and extract the fractional atomic-coordinates.
It is assumed that a GALLOP Structure object has been created
that already contains the correct unit cell and PXRD Pawley data.</p>
<p>These coordinates are added to the Structure object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Structure</code></strong> :&ensp;<code>class</code></dt>
<dd>GALLOP structure object</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the CIF. Assumed to be in the working
directory.</dd>
<dt><strong><code>add_CIF_dw_factors</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Include Debye-Waller factors
for the atoms in the CIF. By default, non-H = 3, H = 6.
Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_CIF_atomic_coords_and_species(Structure, filename,
                                        add_CIF_dw_factors=True):
    &#34;&#34;&#34;
    Read a CIF and extract the fractional atomic-coordinates.
    It is assumed that a GALLOP Structure object has been created
    that already contains the correct unit cell and PXRD Pawley data.

    These coordinates are added to the Structure object.
    Args:
        Structure (class): GALLOP structure object
        filename (str): Filename of the CIF. Assumed to be in the working
            directory.
        add_CIF_dw_factors (bool, optional): Include Debye-Waller factors
            for the atoms in the CIF. By default, non-H = 3, H = 6.
            Defaults to True.
    &#34;&#34;&#34;

    print(&#34;Ensure that CIF and structure fit files are in the same setting&#34;)
    cif_file_structure = cif.CifParser(filename).as_dict()
    cif_file_structure = cif_file_structure[list(cif_file_structure.keys())[0]]
    Structure.cif_species = cif_file_structure[&#34;_atom_site_type_symbol&#34;]
    x = cif_file_structure[&#34;_atom_site_fract_x&#34;]
    y = cif_file_structure[&#34;_atom_site_fract_y&#34;]
    z = cif_file_structure[&#34;_atom_site_fract_z&#34;]

    x = np.array([v.split(&#34;(&#34;)[0] for v in x]).astype(float)
    y = np.array([v.split(&#34;(&#34;)[0] for v in y]).astype(float)
    z = np.array([v.split(&#34;(&#34;)[0] for v in z]).astype(float)
    Structure.cif_frac_coords = np.vstack([x,y,z]).T
    if add_CIF_dw_factors:
        elements = list(set(Structure.cif_species))
        Structure.CIF_dw_factors = {}
        for e in elements:
            if e != &#34;H&#34;:
                Structure.CIF_dw_factors[e] = 3.
            else:
                Structure.CIF_dw_factors[e] = 6.
    no_H_coords = []
    no_H_species = []
    if Structure.ignore_H_atoms:
        for i, e in enumerate(cif_file_structure[&#34;_atom_site_type_symbol&#34;]):
            if e != &#34;H&#34;:
                no_H_species.append(e)
                no_H_coords.append(Structure.cif_frac_coords[i])
        Structure.cif_species_no_H = no_H_species
        Structure.cif_frac_coords_no_H = np.vstack(no_H_coords)</code></pre>
</details>
</dd>
<dt id="gallop.files.get_DASH_inverse_covariance_matrix"><code class="name flex">
<span>def <span class="ident">get_DASH_inverse_covariance_matrix</span></span>(<span>off_diag_elements, sigma, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in a DASH-produced list of correlated peaks and produce
a 2D numpy inverse covariance matrix.
-
Populate the diagonals with sigma^2
-
Populate the off-diagonals as raw decimals rather than
percentages to save having to do it later.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>off_diag_elements</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Non-zero off-diagonal elements
of the inverse correlation matrix stored in the hcv, which
will be converted into the covariance matrix</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>The square-root of the diagonal elements of
the inverse covariance matrix</dd>
<dt><strong><code>percentage_cutoff_inv_cov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the minimum percentage
correlation to be included in the inverse covariance matrix.
Defaults to 20 to be comparable with DASH, however, this doesn't
affect the speed of GALLOP so can be freely set without impacting
performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy array</code></dt>
<dd>The inverse covariance matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_DASH_inverse_covariance_matrix(
                    off_diag_elements, sigma, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH-produced list of correlated peaks and produce
    a 2D numpy inverse covariance matrix.
        -   Populate the diagonals with sigma^2
        -   Populate the off-diagonals as raw decimals rather than
            percentages to save having to do it later.

    Args:
        off_diag_elements (numpy array): Non-zero off-diagonal elements
            of the inverse correlation matrix stored in the hcv, which
            will be converted into the covariance matrix
        sigma (numpy array): The square-root of the diagonal elements of
            the inverse covariance matrix
        percentage_cutoff_inv_cov (int, optional): the minimum percentage
            correlation to be included in the inverse covariance matrix.
            Defaults to 20 to be comparable with DASH, however, this doesn&#39;t
            affect the speed of GALLOP so can be freely set without impacting
            performance.

    Returns:
        numpy array: The inverse covariance matrix
    &#34;&#34;&#34;
    matrix = np.zeros((len(off_diag_elements), len(off_diag_elements)))
    for i in range(0,len(off_diag_elements)):
        if i != len(off_diag_elements)-1:
            j = i+1
            # Populate diagonals
            matrix[i][i] += sigma[i]**2
            # Populate off diagonal elements
            for corr in off_diag_elements[i]:
                if float(corr) &gt;= percentage_cutoff_inv_cov:
                    matrix[i][j] = 0.01*sigma[i]*sigma[j]*float(corr)
                    matrix[j][i] += matrix[i][j]
                j+=1
        else:
            matrix[i][i] += sigma[i]**2
    return matrix</code></pre>
</details>
</dd>
<dt id="gallop.files.get_GSAS_inv_cov_matrix"><code class="name flex">
<span>def <span class="ident">get_GSAS_inv_cov_matrix</span></span>(<span>cov_subset, names_all, names_subset, I, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverts the covariance matrix from GSAS, and use this to rebuild a full
inverse covariance matrix that includes the hkls that were excluded in
the GSAS-II Pawley fitting procedure (due to overlap/equivalence etc)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cov_subset</code></strong> :&ensp;<code>np.array</code></dt>
<dd>The covariance matrix from GSAS. This will be of
shape names_subset x names_subset</dd>
<dt><strong><code>names_all</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of all of the peaks within the data range used
for the Pawley fit in GSAS</dd>
<dt><strong><code>names_subset</code></strong> :&ensp;<code>list</code></dt>
<dd>The non-equivalent peaks used by GSAS to perform
the Pawley refinement</dd>
<dt><strong><code>percentage_cutoff_inv_cov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the minimum percentage
correlation to be included in the inverse
covariance matrix. Defaults to 20 to be comparable to
DASH. There is no difference in speed between 0 and 20
(or any other number from 0-100) so can be tested freely
without impacting run times.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>The modified intensity for all hkls</dd>
<dt><code>np.array</code></dt>
<dd>The full inverse covariance matrix for all hkls</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_GSAS_inv_cov_matrix(cov_subset, names_all, names_subset, I,
                                                percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Inverts the covariance matrix from GSAS, and use this to rebuild a full
    inverse covariance matrix that includes the hkls that were excluded in
    the GSAS-II Pawley fitting procedure (due to overlap/equivalence etc)

    Args:
        cov_subset (np.array): The covariance matrix from GSAS. This will be of
                        shape names_subset x names_subset
        names_all (list): A list of all of the peaks within the data range used
                        for the Pawley fit in GSAS
        names_subset (list): The non-equivalent peaks used by GSAS to perform
                        the Pawley refinement
        percentage_cutoff_inv_cov (int, optional):  the minimum percentage
                        correlation to be included in the inverse
                        covariance matrix. Defaults to 20 to be comparable to
                        DASH. There is no difference in speed between 0 and 20
                        (or any other number from 0-100) so can be tested freely
                        without impacting run times.

    Returns:
        np.array: The modified intensity for all hkls
        np.array: The full inverse covariance matrix for all hkls
    &#34;&#34;&#34;

    # Find the peaks not in the Pawley refinement and account for any other
    # refined variables, as we only want the peak-to-peak covariance
    peak_indices = [i for i, x in enumerate(names_subset) if &#34;PWLref&#34; in x]

    names_all = [x for x in names_all if &#34;PWL&#34; in x]
    names_subset = [x for x in names_subset if &#34;PWL&#34; in x]
    missing_peaks = [x for x in names_all if x not in names_subset]

    # Invert the covariance matrix and convert to the inverse correlation matrix
    inv_cov_subset = np.linalg.inv(cov_subset)
    # Select only the peak-to-peak inverse covariances
    inv_cov_subset = inv_cov_subset[peak_indices][:,peak_indices]
    inv_sigma_subset = np.sqrt(np.diag(inv_cov_subset))
    inv_cor_subset = (np.diag(1/inv_sigma_subset)
                        @ inv_cov_subset
                        @ np.diag(1/inv_sigma_subset))

    # Now we have the inverse correlation rather than covariance matrix,
    # we can build it up to include all of the peaks, then rebuild the inverse
    # covariance matrix from that. We also need to modify the intensities and
    # 1/sigmas of the peaks that were excluded from the Pawley fit.
    # Effectively, this is converting the output to look like a DASH hcv, from
    # which the same logic is then used to rebuild the inverse covariance matrix
    inv_cor_full = []
    inv_sigma_full = []
    I_mod = np.copy(I)
    j=0
    for i, x in enumerate(names_all):
        if x not in missing_peaks:
            correlations = inv_cor_subset[i-j][i-j+1:]
            inv_cor_full.append(np.around(correlations, 2).tolist())
            inv_sigma_full.append(inv_sigma_subset[i-j])
        else:
            inv_sigma_full.append(None)
            inv_cor_full.append(None)
            j+=1

    for i, s in enumerate(inv_sigma_full):
        if s is None:
            inv_sigma_full[i] = inv_sigma_full[i-1]

    inv_sigma_mod = np.copy(inv_sigma_full)
    for i, c in enumerate(inv_cor_full):
        if c is not None:
            k = 0
            if i &lt; (len(inv_cor_full)-1):
                while inv_cor_full[i+k+1] is None:
                    if i + k + 2 == len(inv_cor_full):
                        break
                    k+=1
                if k &gt; 0:
                    base_correlations = inv_cor_full[i]
                    for j in range(k+1):
                        inv_cor_full[i+j] = (k-j)*[1.] + base_correlations
                        # Divide the intensity of the overlapped peaks by the
                        # number of peaks that were overlapped
                        I_mod[i+j] = I_mod[i+j] / (k+1.)
                        # Multiply the inverse sigma of the overlapped peaks by
                        # the number of peaks that were overlapped
                        inv_sigma_mod[i+j] = inv_sigma_mod[i+j] * (k+1.)

    # Now reconstruct the full inverse correlation matrix
    inv_cor_matrix = np.zeros((len(inv_cor_full), len(inv_cor_full)))
    for i in range(0,len(inv_cor_full)):
        if i != len(inv_cor_full)-1:
            j = i+1
            # Populate diagonals
            inv_cor_matrix[i][i] = 1.
            # Populate off diagonal elements
            for corr in inv_cor_full[i]:
                if np.abs(corr) &gt;= (percentage_cutoff_inv_cov/100.):
                    inv_cor_matrix[i][j] = float(corr)
                    inv_cor_matrix[j][i] = inv_cor_matrix[i][j]
                j+=1
        else:
            inv_cor_matrix[i][i] = 1.

    # Finally, get the inverse covariance matrix needed for chi2 calcs.
    inverse_covariance_matrix = (np.diag(inv_sigma_mod)
                                @ inv_cor_matrix
                                @ np.diag(inv_sigma_mod))
    return I_mod, inverse_covariance_matrix</code></pre>
</details>
</dd>
<dt id="gallop.files.get_TOPAS_matrix"><code class="name flex">
<span>def <span class="ident">get_TOPAS_matrix</span></span>(<span>lines)</span>
</code></dt>
<dd>
<div class="desc"><p>Read some lines from a TOPAS output file that correspond to
a Covariance matrix and return a numpy array of the matrix</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>lines</code></strong> :&ensp;<code>List</code></dt>
<dd>List of the lines in a TOPAS output file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy array</code></dt>
<dd>The topas covariance matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_TOPAS_matrix(lines):
    &#34;&#34;&#34;
    Read some lines from a TOPAS output file that correspond to
    a Covariance matrix and return a numpy array of the matrix

    Args:
        lines (List): List of the lines in a TOPAS output file

    Returns:
        numpy array: The topas covariance matrix
    &#34;&#34;&#34;
    matrix = []
    for line in lines:
        temp = []
        for item in line[2:]:
            if &#34;iprm&#34; in line[0]:
                try:
                    temp.append(float(item))
                except ValueError:
                    minus_idx = [i for i, ltr in enumerate(item) if ltr == &#34;-&#34;]
                    split = item.split(&#34;-&#34;)
                    if len(minus_idx) &gt; 1:
                        temp.append(-1*float(split[1]))
                        temp.append(-1*float(split[2]))
                    else:
                        temp.append(float(split[0]))
                        temp.append(-1*float(split[1]))
        matrix.append(temp)
    return np.array(matrix)</code></pre>
</details>
</dd>
<dt id="gallop.files.get_data_from_DASH_sdi"><code class="name flex">
<span>def <span class="ident">get_data_from_DASH_sdi</span></span>(<span>filename, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in a DASH .sdi file to obtain numpy arrays of:
- hkls
- twotheta for hkls
- peak intensities
- inverse covariance matrix
As well as:
- pymatgen lattice
- pymatgen Space group</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filename of the .sdi to read in. The associated
hcv, dsl and tic files should also be in the same directory and
accessible.</dd>
<dt><strong><code>percentage_cutoff</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the minimum percentage correlation
to be included in the inverse covariance matrix.
Defaults to 20 to be comparable with DASH, however, this
doesn't affect the speed of GALLOP on GPU so can be
freely set without impacting performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dictionary with the required information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_from_DASH_sdi(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH .sdi file to obtain numpy arrays of:
        - hkls
        - twotheta for hkls
        - peak intensities
        - inverse covariance matrix
    As well as:
        - pymatgen lattice
        - pymatgen Space group

    Args:
        filename (str): The filename of the .sdi to read in. The associated
            hcv, dsl and tic files should also be in the same directory and
            accessible.
        percentage_cutoff (int, optional):  the minimum percentage correlation
                        to be included in the inverse covariance matrix.
                        Defaults to 20 to be comparable with DASH, however, this
                        doesn&#39;t affect the speed of GALLOP on GPU so can be
                        freely set without impacting performance.
    Returns:
        dict: A dictionary with the required information
    &#34;&#34;&#34;
    data = {}
    data[&#34;hcv&#34;], data[&#34;tic&#34;], data[&#34;dsl&#34;], data[&#34;unit_cell&#34;], \
                data[&#34;sg_number&#34;], data[&#34;sg_setting&#34;] = read_DASH_sdi(filename)
    data[&#34;wavelength&#34;] = read_DASH_dsl(data[&#34;dsl&#34;])
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    data[&#34;hkl&#34;], data[&#34;intensities&#34;], data[&#34;sigma&#34;],\
        data[&#34;inverse_covariance_matrix&#34;] = read_DASH_hcv(
            data[&#34;hcv&#34;], percentage_cutoff_inv_cov=percentage_cutoff_inv_cov)
    data[&#34;twotheta&#34;] = read_DASH_tic(data[&#34;tic&#34;])
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(data[&#34;unit_cell&#34;][0],
                                                    data[&#34;unit_cell&#34;][1],
                                                    data[&#34;unit_cell&#34;][2],
                                                    data[&#34;unit_cell&#34;][3],
                                                    data[&#34;unit_cell&#34;][4],
                                                    data[&#34;unit_cell&#34;][5])
    data[&#34;space_group&#34;] = groups.SpaceGroup.from_int_number(data[&#34;sg_number&#34;])
    return data</code></pre>
</details>
</dd>
<dt id="gallop.files.get_data_from_GSAS_gpx"><code class="name flex">
<span>def <span class="ident">get_data_from_GSAS_gpx</span></span>(<span>filename, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in a GSAS project file (.gpx) to obtain numpy arrays of:
- hkls
- peak intensities
- inverse covariance matrix
As well as:
- pymatgen lattice
- pymatgen Space group</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
<dt><strong><code>percentage_cutoff_inv_cov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the minimum percentage
correlation to be included in the inverse
covariance matrix. Defaults to 20 to be
comparable with DASH, however, this doesn't
affect the speed of GALLOP so can be freely
set without impacting performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dictionary with the required information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_from_GSAS_gpx(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a GSAS project file (.gpx) to obtain numpy arrays of:
        - hkls
        - peak intensities
        - inverse covariance matrix
    As well as:
        - pymatgen lattice
        - pymatgen Space group

    Args:
        filename (str): [description]
        percentage_cutoff_inv_cov (int, optional):  the minimum percentage
                        correlation to be included in the inverse
                        covariance matrix. Defaults to 20 to be
                        comparable with DASH, however, this doesn&#39;t
                        affect the speed of GALLOP so can be freely
                        set without impacting performance.
    Returns:
        dict: A dictionary with the required information
    &#34;&#34;&#34;
    data, names = IndexGPX(filename)

    instrument_params = data[names[-2][0]][names[-2][4]][0]
    if &#34;Lam&#34; in instrument_params.keys():
        wavelength = instrument_params[&#34;Lam&#34;][1]
    elif &#34;Lam1&#34; in instrument_params.keys():
        wavelength = instrument_params[&#34;Lam1&#34;][1]

    space_group = data[&#34;Phases&#34;][names[-1][-1]][&#34;General&#34;][&#34;SGData&#34;][&#34;SpGrp&#34;]
    cell = data[&#34;Phases&#34;][names[-1][-1]][&#34;General&#34;][&#34;Cell&#34;][1:-1]

    hkl, dspacing, I, sigma = [], [], [], []
    for d in data[&#34;Phases&#34;][names[-1][-1]][&#34;Pawley ref&#34;]:
        hkl.append(d[:3])
        dspacing.append(d[4])
        I.append(d[-2])
        sigma.append(d[-1])

    hkl = np.array(hkl).astype(int)
    dspacing = np.array(dspacing)
    I = np.array(I)
    sigma = np.array(sigma)

    names_subset = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;varyList&#34;]
    names_all = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;varyListStart&#34;]
    cov_subset = data[&#34;Covariance&#34;][&#34;data&#34;][&#34;covMatrix&#34;]

    intensities, inverse_covariance_matrix = get_GSAS_inv_cov_matrix(cov_subset,
                            names_all, names_subset, I,
                            percentage_cutoff_inv_cov=percentage_cutoff_inv_cov)
    data = {}
    data[&#34;wavelength&#34;] = wavelength
    data[&#34;hkl&#34;] = hkl
    data[&#34;intensities&#34;] = intensities
    data[&#34;dspacing&#34;] = dspacing
    data[&#34;inverse_covariance_matrix&#34;] = inverse_covariance_matrix
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(cell[0], cell[1], cell[2],
                                                    cell[3], cell[4], cell[5])
    data[&#34;space_group&#34;] = pmg.symmetry.groups.SpaceGroup(space_group)
    data[&#34;sg_number&#34;] = int(data[&#34;space_group&#34;].int_number)
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    return data</code></pre>
</details>
</dd>
<dt id="gallop.files.get_data_from_TOPAS_output"><code class="name flex">
<span>def <span class="ident">get_data_from_TOPAS_output</span></span>(<span>filename, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Note: This assumes that the only refined parameters in the Pawley refinement
are the peak intensities.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
<dt><strong><code>percentage_cutoff_inv_cov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 20</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dictionary of the required information</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_from_TOPAS_output(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Note: This assumes that the only refined parameters in the Pawley refinement
    are the peak intensities.
    Args:
        filename ([type]): [description]
        percentage_cutoff_inv_cov (int, optional): [description]. Defaults to 20

    Returns:
        dict: A dictionary of the required information
    &#34;&#34;&#34;

    lines = []
    with open(filename, &#34;r&#34;) as in_file:
        for line in in_file:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if len(line) &gt; 0:
                lines.append(line)
    in_file.close()

    cell = {}
    hkl = []
    m = []
    dspacing = []
    twotheta = []
    I = []
    #sigma = []
    C_matrix = []
    gof = 1
    for i, line in enumerate(lines):
        if &#34;la&#34; in line and &#34;1&#34; in line and &#34;lo&#34; in line:
            wavelength = line[-1]
        if len(line) == 2:
            if &#34;a&#34; in line:
                cell[&#34;a&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;b&#34; in line:
                cell[&#34;b&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;c&#34; in line:
                cell[&#34;c&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;al&#34; in line:
                cell[&#34;al&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;be&#34; in line:
                cell[&#34;be&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
            if &#34;ga&#34; in line:
                cell[&#34;ga&#34;] = float(line[1].split(&#34;_&#34;)[0].strip(&#34;`&#34;))
        if &#34;space_group&#34; in line:
            cell[&#34;space_group&#34;] = &#34; &#34;.join(line[1:]).strip(&#34;\&#34;&#34;)
        if &#34;gof&#34; in line:
            gof *= float(line[1])
        if len(line) == 8 and &#34;@&#34; in line:
            hkl.append([int(x) for x in line[0:3]])
            m.append(int(line[3]))
            dspacing.append(line[4])
            twotheta.append(line[5])
            I_sig = line[-1].split(&#34;`_&#34;)
            I.append(float(I_sig[0]))
            #sigma.append(float(I_sig[1]))
        else:
            if &#34;C_matrix&#34; in line:
                n_peaks = int(lines[i+2][-1])
                C_matrix = get_TOPAS_matrix(lines[i+3:i+2+n_peaks+1])

    hkl = np.array(hkl)
    m = np.array(m)
    dspacing = np.array(dspacing)
    twotheta = np.array(twotheta)
    I = np.array(I)
    I_mult_corrected = I/m
    C_matrix_mult_corrected = np.diag(1/m) @ C_matrix @ np.diag(1/m)
    #npeaks = len(hkl)
    #while not np.all(
    #    np.linalg.eigvals(
    #    np.linalg.inv(C_matrix_mult_corrected[:npeaks][:,:npeaks])) &gt; 0):
    #    npeaks -= 1
    #if npeaks &lt; len(hkl):
    #    print(&#34;WARNING - inverse covariance matrix is not positive definite.&#34;)
    #    print(&#34;Recommend reducing peak count to&#34;,npeaks)
    #    print(&#34;This is&#34;,round(100*float(npeaks)/len(hkl),2),&#34;% of the peaks &#34;
    #            &#34;in the Pawley file&#34;)

    ############################################################################
    # Still need to work out how to implement the % correlation cutoff
    # It&#39;s proving difficult because the matrices from TOPAS I&#39;ve got give
    # negative values on the inverse matrix diagonal which prevents me from
    # converting to the inverse correlation matrix.
    ############################################################################
    data = {}
    data[&#34;percentage_cutoff_inv_cov&#34;] = percentage_cutoff_inv_cov
    data[&#34;hkl&#34;] = hkl.astype(int)
    data[&#34;intensities&#34;] = I_mult_corrected
    data[&#34;inverse_covariance_matrix&#34;] = np.linalg.inv(C_matrix_mult_corrected)
    data[&#34;lattice&#34;] = pmg.Lattice.from_parameters(cell[&#34;a&#34;],
                                                    cell[&#34;b&#34;],
                                                    cell[&#34;c&#34;],
                                                    cell[&#34;al&#34;],
                                                    cell[&#34;be&#34;],
                                                    cell[&#34;ga&#34;])
    space_group = pmg.symmetry.groups.SpaceGroup(cell[&#34;space_group&#34;])

    data[&#34;sg_number&#34;] = int(space_group.int_number)
    data[&#34;original_sg_number&#34;] = data[&#34;sg_number&#34;]
    data[&#34;space_group&#34;] = space_group
    data[&#34;dspacing&#34;] = dspacing
    data[&#34;twotheta&#34;] = twotheta
    data[&#34;wavelength&#34;] = wavelength
    return data</code></pre>
</details>
</dd>
<dt id="gallop.files.read_DASH_dsl"><code class="name flex">
<span>def <span class="ident">read_DASH_dsl</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Read a DASH dsl file to obtain the wavelength used</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the dsl file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the wavelength used</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_DASH_dsl(filename):
    &#34;&#34;&#34;
    Read a DASH dsl file to obtain the wavelength used

    Args:
        filename (str): Filename of the dsl file

    Returns:
        float: the wavelength used
    &#34;&#34;&#34;
    with open(filename) as in_dsl:
        for line in in_dsl:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if &#34;rad&#34; in line:
                return float(line[1])</code></pre>
</details>
</dd>
<dt id="gallop.files.read_DASH_hcv"><code class="name flex">
<span>def <span class="ident">read_DASH_hcv</span></span>(<span>filename, percentage_cutoff_inv_cov=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in a DASH-produced hcv, and return numpy arrays of hkl,
intensity, sigma values together with a 2D numpy inverse_covariance
matrix.
Format of DASH hcv files:
h, k, l, I, sigma, count, off-diag inv. of correlation matrix
See function get_DASH_inverse_covariance_matrix for how the
inverse_covariance_matrix is populated using the sigma and
inverse correlation matrix values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name for the DASH .sdi file. File is assumed
to be in the working directory</dd>
<dt><strong><code>percentage_cutoff_inv_cov</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the minimum percentage
correlation to be included in the inverse covariance
matrix. Defaults to 20 to be comparable with DASH,
however, this doesn't affect the speed of GALLOP so can
be set as desired without impacting performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Numpy arrays of hkl, intensity, sigma and the inverse
covariance matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_DASH_hcv(filename, percentage_cutoff_inv_cov=20):
    &#34;&#34;&#34;
    Read in a DASH-produced hcv, and return numpy arrays of hkl,
    intensity, sigma values together with a 2D numpy inverse_covariance
    matrix.
    Format of DASH hcv files:
    h, k, l, I, sigma, count, off-diag inv. of correlation matrix
    See function get_DASH_inverse_covariance_matrix for how the
    inverse_covariance_matrix is populated using the sigma and
    inverse correlation matrix values.

    Args:
        filename (str): Name for the DASH .sdi file. File is assumed
            to be in the working directory
        percentage_cutoff_inv_cov (int, optional): the minimum percentage
            correlation to be included in the inverse covariance
            matrix. Defaults to 20 to be comparable with DASH,
            however, this doesn&#39;t affect the speed of GALLOP so can
            be set as desired without impacting performance.

    Returns:
        tuple:  Numpy arrays of hkl, intensity, sigma and the inverse
                covariance matrix
    &#34;&#34;&#34;

    hkl = []
    I = []
    sigma = []
    inverse_covariance_off_diag = []
    peaknumbers = []
    with open(filename) as in_hcv:
        for line in in_hcv:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            hkl.append(line[0:3])
            I.append(line[3])
            sigma.append(line[4])
            peaknumbers.append(int(line[5]))
            inverse_covariance_off_diag.append(line[6:])
    in_hcv.close()
    hkl = np.array(hkl).astype(int)
    I = np.array(I).astype(float)
    sigma = np.array(sigma).astype(float)
    peaknumbers = np.array(peaknumbers)
    inverse_covariance = get_DASH_inverse_covariance_matrix(
                                inverse_covariance_off_diag,
                                sigma,
                                percentage_cutoff_inv_cov=percentage_cutoff_inv_cov
                                )
    return hkl, I, sigma, inverse_covariance</code></pre>
</details>
</dd>
<dt id="gallop.files.read_DASH_sdi"><code class="name flex">
<span>def <span class="ident">read_DASH_sdi</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Read a DASH .sdi file, and obtain the filenames for the the .hcv
and .tic, as well as the unit cell lattice parameters and the
space group.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the DASH .sdi file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>contains the filenames of hcv, tic files as well as
the unit cell, spacegroup and its setting.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_DASH_sdi(filename):
    &#34;&#34;&#34;
    Read a DASH .sdi file, and obtain the filenames for the the .hcv
    and .tic, as well as the unit cell lattice parameters and the
    space group.

    Args:
        filename (str): Filename of the DASH .sdi file

    Returns:
        tuple: contains the filenames of hcv, tic files as well as
            the unit cell, spacegroup and its setting.
    &#34;&#34;&#34;
    directory = os.path.split(filename)[0]
    with open(filename) as in_sdi:
        for line in in_sdi:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            if line[0] == &#34;HCV&#34;:
                hcv = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;TIC&#34;:
                tic = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;DSL&#34;:
                dsl = os.path.join(os.getcwd(),directory,line[1].strip(&#34;.\\&#34;))
            if line[0] == &#34;Cell&#34;:
                unit_cell = np.array(line[1:]).astype(float)
            if line[0] == &#34;SpaceGroup&#34;:
                sg = int(line[2].split(&#34;:&#34;)[0])
                sg_setting = line[2].split(&#34;:&#34;)[-1]
    in_sdi.close()
    return hcv, tic, dsl, unit_cell, sg, sg_setting</code></pre>
</details>
</dd>
<dt id="gallop.files.read_DASH_tic"><code class="name flex">
<span>def <span class="ident">read_DASH_tic</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Read in a DASH-produced tic file, and return numpy array of
twotheta values</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of the .tic file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Numpy array</code></dt>
<dd>the two-theta values for each hkl</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_DASH_tic(filename):
    &#34;&#34;&#34;
    Read in a DASH-produced tic file, and return numpy array of
    twotheta values

    Args:
        filename (str): Filename of the .tic file

    Returns:
        Numpy array: the two-theta values for each hkl
    &#34;&#34;&#34;
    twotheta = []
    with open(filename) as in_tic:
        for line in in_tic:
            line = list(filter(None,line.strip().split(&#34; &#34;)))
            twotheta.append(line[3])
    in_tic.close()
    twotheta = np.array(twotheta).astype(float)
    return twotheta</code></pre>
</details>
</dd>
<dt id="gallop.files.save_CIF_of_best_result"><code class="name flex">
<span>def <span class="ident">save_CIF_of_best_result</span></span>(<span>Structure, result, start_time=None, n_reflections=None, filename_root=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Save a CIF of the best result obtained in a minimise run.
Filename contains information about the run. For example, if the result
is the first run, and the best chi_2 value is 50.1, 300 reflections were
used in the chi_2 calculation and the time taken to obtain this solution was
1.2 minutes, then the filename would be:</p>
<pre><code>filename_root_001_50.1_chisqd_300_refs_1.2_mins.cif
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Structure</code></strong> :&ensp;<code>class</code></dt>
<dd>GALLOP structure object</dd>
<dt><strong><code>result</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of results obtained by minimise</dd>
<dt><strong><code>start_time</code></strong> :&ensp;<code>time</code></dt>
<dd>time.time() when the runs began</dd>
<dt><strong><code>n_reflections</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of reflections used for chi_2</dd>
<dt>calc. Defaults to None.</dt>
<dt><strong><code>filename_root</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If None, takes the name from the
Structure. Root of the filename can be overwritten with this
argument. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_CIF_of_best_result(Structure, result, start_time=None,
    n_reflections=None, filename_root=None):
    &#34;&#34;&#34;
    Save a CIF of the best result obtained in a minimise run.
    Filename contains information about the run. For example, if the result
    is the first run, and the best chi_2 value is 50.1, 300 reflections were
    used in the chi_2 calculation and the time taken to obtain this solution was
    1.2 minutes, then the filename would be:

        filename_root_001_50.1_chisqd_300_refs_1.2_mins.cif

    Args:
        Structure (class): GALLOP structure object
        result (dict): Dictionary of results obtained by minimise
        start_time (time): time.time() when the runs began
        n_reflections (int, optional): the number of reflections used for chi_2
        calc. Defaults to None.
        filename_root (str, optional): If None, takes the name from the
            Structure. Root of the filename can be overwritten with this
            argument. Defaults to None.
    &#34;&#34;&#34;
    external = result[&#34;external&#34;]
    internal = result[&#34;internal&#34;]
    chi_2 = result[&#34;chi_2&#34;]
    run = result[&#34;GALLOP Iter&#34;] + 1
    if n_reflections is None:
        n_reflections = len(Structure.hkl)
    elif n_reflections &gt; len(Structure.hkl):
        n_reflections = len(Structure.hkl)
    # For the purpose of outputting a CIF, include the H-atoms back in. However,
    # the GALLOP runs may not be finished, so save whatever the parameter is set
    # to, then restore this setting once the CIF has been written.
    ignore_H_setting = Structure.ignore_H_atoms

    Structure.ignore_H_atoms = False

    # Save a CIF of the best particle found
    best_frac_coords = zm_to_cart.get_asymmetric_coords_from_numpy(Structure,
            external[chi_2 == chi_2.min()], internal[chi_2 == chi_2.min()],
            verbose=False)

    species = []
    for zm in Structure.zmatrices:
        species += zm.elements

    output_structure = pmg.Structure(lattice=Structure.lattice, species=species,
                                    coords=best_frac_coords[0][:len(species)])
    ext_comment = &#34;# GALLOP External Coords = &#34; + &#34;,&#34;.join(list(
                                external[chi_2 == chi_2.min()][0].astype(str)))
    int_comment = &#34;# GALLOP Internal Coords = &#34; + &#34;,&#34;.join(list(
                                internal[chi_2 == chi_2.min()][0].astype(str)))
    comment = ext_comment + &#34;\n&#34; + int_comment
    writer = DASHCifWriter(output_structure, symprec=1e-12,
                            sg_number=Structure.original_sg_number,
                            comment=comment)

    if filename_root is None:
        filename_root = Structure.name
    if start_time is not None:
        filename = (filename_root
                + &#34;_{:04d}_{:.3f}_chisqd_{}_refs_{:.1f}_mins.cif&#34;)
        filename = filename.format(run, chi_2.min(), n_reflections,
                            (time.time()-start_time)/60)
    else:
        filename = (filename_root
                + &#34;_{:04d}_{:.3f}_chisqd_{}_refs.cif&#34;)
        filename = filename.format(run, chi_2.min(), n_reflections)
    ciffile = writer.cf.data[list(writer.cf.data.keys())[0]]
    # Add the filename to the data_ string in the CIF to make it easier to
    # navigate multiple structures in Mercury
    ciffile.header = filename
    writer.cf.data = {filename : ciffile}
    writer.write_file(filename)
    # Restore the Structure ignore_H_atoms setting
    Structure.ignore_H_atoms = ignore_H_setting</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gallop.files.DASHCifWriter"><code class="flex name class">
<span>class <span class="ident">DASHCifWriter</span></span>
<span>(</span><span>struct, symprec=None, significant_figures=8, sg_number=1, comment=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A modified wrapper around the pymatgen CifFile to write CIFs from pymatgen
structures.</p>
<p>See "CifWriter" on this page for the original code:
<a href="https://pymatgen.org/pymatgen.io.cif.html">https://pymatgen.org/pymatgen.io.cif.html</a></p>
<p>Modification is to stop the unit cell (and hence coordinates) from being
"standardised". This means that the output CIF is in the same setting as
the input data which allows for easier comparison etc.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>struct</code></strong> :&ensp;<code>Structure</code></dt>
<dd>structure to write</dd>
<dt><strong><code>symprec</code></strong> :&ensp;<code>float</code></dt>
<dd>If not none, finds the symmetry of the structure
and writes the cif with symmetry information. Passes symprec
to the SpacegroupAnalyzer.</dd>
<dt><strong><code>significant_figures</code></strong> :&ensp;<code>int</code></dt>
<dd>Specifies precision for formatting of
floats. Defaults to 8.</dd>
<dt><strong><code>angle_tolerance</code></strong> :&ensp;<code>float</code></dt>
<dd>Angle tolerance for symmetry finding.
Passes angle_tolerance to the SpacegroupAnalyzer.
Used only if symprec is not None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DASHCifWriter:
    &#34;&#34;&#34;
    A modified wrapper around the pymatgen CifFile to write CIFs from pymatgen
    structures.

    See &#34;CifWriter&#34; on this page for the original code:
    https://pymatgen.org/pymatgen.io.cif.html

    Modification is to stop the unit cell (and hence coordinates) from being
    &#34;standardised&#34;. This means that the output CIF is in the same setting as
    the input data which allows for easier comparison etc.
    &#34;&#34;&#34;
    def __init__(self, struct, symprec=None, significant_figures=8, sg_number=1,
                comment=None):
        &#34;&#34;&#34;
        Args:
            struct (Structure): structure to write
            symprec (float): If not none, finds the symmetry of the structure
                and writes the cif with symmetry information. Passes symprec
                to the SpacegroupAnalyzer.
            significant_figures (int): Specifies precision for formatting of
                floats. Defaults to 8.
            angle_tolerance (float): Angle tolerance for symmetry finding.
                Passes angle_tolerance to the SpacegroupAnalyzer.
                Used only if symprec is not None.
        &#34;&#34;&#34;

        format_str = &#34;{:.%df}&#34; % significant_figures

        block = OrderedDict()
        loops = []

        spacegroup = (pmg.symmetry.groups.sg_symbol_from_int_number(sg_number),
                        str(sg_number))

        latt = struct.lattice
        comp = struct.composition
        no_oxi_comp = comp.element_composition
        block[&#34;_symmetry_space_group_name_H-M&#34;] = spacegroup[0]
        for cell_attr in [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]:
            block[&#34;_cell_length_&#34; + cell_attr] = format_str.format(
                getattr(latt, cell_attr))
        for cell_attr in [&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;]:
            block[&#34;_cell_angle_&#34; + cell_attr] = format_str.format(
                getattr(latt, cell_attr))
        block[&#34;_symmetry_Int_Tables_number&#34;] = spacegroup[1]
        block[&#34;_chemical_formula_structural&#34;] = no_oxi_comp.reduced_formula
        block[&#34;_chemical_formula_sum&#34;] = no_oxi_comp.formula
        block[&#34;_cell_volume&#34;] = format_str.format(latt.volume)

        _, fu = no_oxi_comp.get_reduced_composition_and_factor()
        block[&#34;_cell_formula_units_Z&#34;] = str(int(fu))

        if symprec is None:
            block[&#34;_symmetry_equiv_pos_site_id&#34;] = [&#34;1&#34;]
            block[&#34;_symmetry_equiv_pos_as_xyz&#34;] = [&#34;x, y, z&#34;]
        else:
            symmops = []
            sg_symbol = pmg.symmetry.groups.sg_symbol_from_int_number(sg_number)
            SG = pmg.symmetry.groups.SpaceGroup(sg_symbol)
            for op in SG.symmetry_ops:
                v = op.translation_vector
                symmops.append(SymmOp.from_rotation_and_translation(
                    op.rotation_matrix, v))

            ops = [op.as_xyz_string() for op in symmops]
            block[&#34;_symmetry_equiv_pos_site_id&#34;] = \
                [&#34;%d&#34; % i for i in range(1, len(ops) + 1)]
            block[&#34;_symmetry_equiv_pos_as_xyz&#34;] = ops

        loops.append([&#34;_symmetry_equiv_pos_site_id&#34;,
                    &#34;_symmetry_equiv_pos_as_xyz&#34;])

        try:
            symbol_to_oxinum = OrderedDict([
                (el.__str__(),
                float(el.oxi_state))
                for el in sorted(comp.elements)])
            block[&#34;_atom_type_symbol&#34;] = symbol_to_oxinum.keys()
            block[&#34;_atom_type_oxidation_number&#34;] = symbol_to_oxinum.values()
            loops.append([&#34;_atom_type_symbol&#34;,
                            &#34;_atom_type_oxidation_number&#34;])
        except (TypeError, AttributeError):
            symbol_to_oxinum = OrderedDict([(el.symbol, 0) for el in
                                            sorted(comp.elements)])

        atom_site_type_symbol = []
        atom_site_symmetry_multiplicity = []
        atom_site_fract_x = []
        atom_site_fract_y = []
        atom_site_fract_z = []
        atom_site_label = []
        atom_site_occupancy = []
        count = 0
        for site in struct:
            for sp, occu in sorted(site.species.items()):
                atom_site_type_symbol.append(sp.__str__())
                atom_site_symmetry_multiplicity.append(&#34;1&#34;)
                atom_site_fract_x.append(format_str.format(site.a))
                atom_site_fract_y.append(format_str.format(site.b))
                atom_site_fract_z.append(format_str.format(site.c))
                atom_site_label.append(&#34;{}{}&#34;.format(sp.symbol, count))
                atom_site_occupancy.append(occu.__str__())
                count += 1

        block[&#34;_atom_site_type_symbol&#34;] = atom_site_type_symbol
        block[&#34;_atom_site_label&#34;] = atom_site_label
        block[&#34;_atom_site_symmetry_multiplicity&#34;] = \
                                                atom_site_symmetry_multiplicity
        block[&#34;_atom_site_fract_x&#34;] = atom_site_fract_x
        block[&#34;_atom_site_fract_y&#34;] = atom_site_fract_y
        block[&#34;_atom_site_fract_z&#34;] = atom_site_fract_z
        block[&#34;_atom_site_occupancy&#34;] = atom_site_occupancy
        loops.append([&#34;_atom_site_type_symbol&#34;,
                    &#34;_atom_site_label&#34;,
                    &#34;_atom_site_symmetry_multiplicity&#34;,
                    &#34;_atom_site_fract_x&#34;,
                    &#34;_atom_site_fract_y&#34;,
                    &#34;_atom_site_fract_z&#34;,
                    &#34;_atom_site_occupancy&#34;])

        d = OrderedDict()
        d[comp.reduced_formula] = CifBlock(block, loops, comp.reduced_formula)
        comment_for_file = &#34;# Generated using pymatgen and GALLOP&#34;
        if comment is not None:
            comment_for_file += &#34;\n&#34; + comment
        self.cf = CifFile(d, comment=comment_for_file)

    @property
    def ciffile(self):
        &#34;&#34;&#34;
        Returns: CifFile associated with the CifWriter.
        &#34;&#34;&#34;
        return self.cf

    def __str__(self):
        &#34;&#34;&#34;
        Returns the cif as a string.
        &#34;&#34;&#34;
        return self.cf.__str__()

    def write_file(self, filename):
        &#34;&#34;&#34;
        Write the cif file.
        &#34;&#34;&#34;
        with zopen(filename, &#34;wt&#34;) as f:
            f.write(self.__str__())</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="gallop.files.DASHCifWriter.ciffile"><code class="name">var <span class="ident">ciffile</span></code></dt>
<dd>
<div class="desc"><p>Returns: CifFile associated with the CifWriter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ciffile(self):
    &#34;&#34;&#34;
    Returns: CifFile associated with the CifWriter.
    &#34;&#34;&#34;
    return self.cf</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gallop.files.DASHCifWriter.write_file"><code class="name flex">
<span>def <span class="ident">write_file</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Write the cif file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_file(self, filename):
    &#34;&#34;&#34;
    Write the cif file.
    &#34;&#34;&#34;
    with zopen(filename, &#34;wt&#34;) as f:
        f.write(self.__str__())</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gallop" href="index.html">gallop</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gallop.files.IndexGPX" href="#gallop.files.IndexGPX">IndexGPX</a></code></li>
<li><code><a title="gallop.files.get_CIF_atomic_coords_and_species" href="#gallop.files.get_CIF_atomic_coords_and_species">get_CIF_atomic_coords_and_species</a></code></li>
<li><code><a title="gallop.files.get_DASH_inverse_covariance_matrix" href="#gallop.files.get_DASH_inverse_covariance_matrix">get_DASH_inverse_covariance_matrix</a></code></li>
<li><code><a title="gallop.files.get_GSAS_inv_cov_matrix" href="#gallop.files.get_GSAS_inv_cov_matrix">get_GSAS_inv_cov_matrix</a></code></li>
<li><code><a title="gallop.files.get_TOPAS_matrix" href="#gallop.files.get_TOPAS_matrix">get_TOPAS_matrix</a></code></li>
<li><code><a title="gallop.files.get_data_from_DASH_sdi" href="#gallop.files.get_data_from_DASH_sdi">get_data_from_DASH_sdi</a></code></li>
<li><code><a title="gallop.files.get_data_from_GSAS_gpx" href="#gallop.files.get_data_from_GSAS_gpx">get_data_from_GSAS_gpx</a></code></li>
<li><code><a title="gallop.files.get_data_from_TOPAS_output" href="#gallop.files.get_data_from_TOPAS_output">get_data_from_TOPAS_output</a></code></li>
<li><code><a title="gallop.files.read_DASH_dsl" href="#gallop.files.read_DASH_dsl">read_DASH_dsl</a></code></li>
<li><code><a title="gallop.files.read_DASH_hcv" href="#gallop.files.read_DASH_hcv">read_DASH_hcv</a></code></li>
<li><code><a title="gallop.files.read_DASH_sdi" href="#gallop.files.read_DASH_sdi">read_DASH_sdi</a></code></li>
<li><code><a title="gallop.files.read_DASH_tic" href="#gallop.files.read_DASH_tic">read_DASH_tic</a></code></li>
<li><code><a title="gallop.files.save_CIF_of_best_result" href="#gallop.files.save_CIF_of_best_result">save_CIF_of_best_result</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gallop.files.DASHCifWriter" href="#gallop.files.DASHCifWriter">DASHCifWriter</a></code></h4>
<ul class="">
<li><code><a title="gallop.files.DASHCifWriter.ciffile" href="#gallop.files.DASHCifWriter.ciffile">ciffile</a></code></li>
<li><code><a title="gallop.files.DASHCifWriter.write_file" href="#gallop.files.DASHCifWriter.write_file">write_file</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>